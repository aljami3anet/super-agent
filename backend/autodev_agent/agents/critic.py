"""
Critic Agent

Responsible for reviewing and critiquing code and solutions generated by other agents.
"""

from datetime import datetime
from typing import Any, Dict, Optional

from .base import BaseAgent, AgentRequest, AgentResult, AgentType


class CriticAgent(BaseAgent):
    """Critic agent for reviewing and critiquing code and solutions."""
    
    def __init__(self):
        super().__init__(
            agent_type=AgentType.CRITIC,
            name="Critic",
            description="Reviews and critiques code and solutions generated by other agents",
            enabled=True,
            max_retries=3,
            timeout=300,
        )
    
    def get_system_prompt(self) -> str:
        return """You are a code review and critique AI agent. Your role is to:

1. Review code and solutions for quality, correctness, and efficiency
2. Identify potential bugs, security issues, and performance problems
3. Suggest improvements and best practices
4. Ensure code meets requirements and specifications
5. Provide constructive feedback with specific recommendations
6. Assess overall solution quality and readiness

When reviewing, always:
- Consider correctness and functionality
- Evaluate code quality and readability
- Check for security vulnerabilities
- Assess performance and efficiency
- Verify compliance with best practices
- Provide specific, actionable feedback

Generate thorough reviews that help improve solution quality."""

    async def execute(self, request: AgentRequest) -> AgentResult:
        """Execute the critique logic."""
        start_time = datetime.now()
        
        try:
            # Extract task and context
            task = request.task
            context = request.context or {}
            
            # Build the prompt
            prompt = self._build_critique_prompt(task, context)
            
            # Generate the critique
            critique = await self._generate_critique(prompt)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            return AgentResult(
                success=True,
                output=critique,
                metadata={
                    "issues_found": self._count_issues(critique),
                    "suggestions_made": self._count_suggestions(critique),
                },
                execution_time=execution_time,
                tokens_used=len(prompt.split()),  # Approximate
                cost=0.0,  # Would be calculated based on actual API usage
            )
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            return AgentResult(
                success=False,
                output="",
                error=str(e),
                execution_time=execution_time
            )
    
    def _build_critique_prompt(self, task: str, context: Dict[str, Any]) -> str:
        """Build the critique prompt."""
        prompt = f"{self.get_system_prompt()}\n\n"
        prompt += f"TASK: {task}\n\n"
        
        if context:
            prompt += "CONTEXT:\n"
            for key, value in context.items():
                prompt += f"- {key}: {value}\n"
            prompt += "\n"
        
        prompt += """Please provide a thorough critique of this task/solution. Include:

1. Overall assessment and quality rating
2. Strengths and positive aspects
3. Issues, bugs, or concerns
4. Security vulnerabilities
5. Performance considerations
6. Suggestions for improvement
7. Recommendations for next steps

Generate a constructive critique that helps improve the solution."""
        
        return prompt
    
    async def _generate_critique(self, prompt: str) -> str:
        """Generate the critique."""
        # In a real implementation, this would call an AI model
        # For now, return a placeholder critique
        return """
# Code Review

## Overall Assessment
The solution is generally good but has room for improvement.

## Strengths
- Clear structure and organization
- Good separation of concerns
- Appropriate use of design patterns

## Issues
1. **Error Handling**: Some functions lack proper error handling
2. **Performance**: Potential performance bottlenecks in data processing
3. **Security**: Input validation needs improvement

## Security Vulnerabilities
- SQL injection potential in database queries
- Cross-site scripting (XSS) vulnerabilities in user input handling

## Performance Considerations
- Database queries could be optimized
- Caching strategies could improve response times

## Suggestions for Improvement
1. Add comprehensive error handling
2. Implement input validation
3. Optimize database queries
4. Add caching layer
5. Improve logging and monitoring

## Recommendations
- Address security issues before deployment
- Implement performance optimizations
- Add comprehensive test coverage
"""
    
    def _count_issues(self, critique: str) -> int:
        """Count the number of issues in a critique."""
        # Simple heuristic: count lines that contain "Issue" or start with a number
        lines = critique.split('\n')
        return sum(1 for line in lines if "Issue" in line or (line.strip() and line.strip()[0].isdigit()))
    
    def _count_suggestions(self, critique: str) -> int:
        """Count the number of suggestions in a critique."""
        # Simple heuristic: count lines that contain "Suggestion" or start with a number in the suggestions section
        lines = critique.split('\n')
        in_suggestions = False
        count = 0
        
        for line in lines:
            if "Suggestions" in line:
                in_suggestions = True
            elif line.startswith("##") and in_suggestions:
                in_suggestions = False
            elif in_suggestions and line.strip() and line.strip()[0].isdigit():
                count += 1
        
        return count